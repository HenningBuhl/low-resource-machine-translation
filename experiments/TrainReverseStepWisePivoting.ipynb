{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src module directory to system path for subsecuent imports.\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa380261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import is_notebook\n",
    "\n",
    "# Settings (only in Jupyter Notebooks).\n",
    "if is_notebook():\n",
    "    # Module reloading.\n",
    "    %load_ext autoreload\n",
    "    # aimport?\n",
    "    %autoreload 2\n",
    "    # Plot settings.\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ca766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_metric\n",
    "\n",
    "from constants import *\n",
    "from util import *\n",
    "from transformer import Transformer\n",
    "from tokenizer import load_tokenizer\n",
    "from data import download_data, load_data\n",
    "from plotting import plot_metric\n",
    "from metric_logging import MetricLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb963cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(0, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment paramereters.\n",
    "hparams = dotdict({\n",
    "    'src_lang': 'de',\n",
    "    'pvt_lang': 'nl',\n",
    "    'tgt_lang': 'en',\n",
    "    'src_pvt_model_path': 'models/baseline-de-nl.pt',\n",
    "    'pvt_tgt_model_path': 'models/baseline-nl-en.pt',\n",
    "    'batch_size': 80,\n",
    "    'max_epochs': 10,\n",
    "    'max_examples': -1,\n",
    "    'max_examples_fine_tune': 10_000,\n",
    "    'gpus': 1,\n",
    "    'num_workers': 4,\n",
    "    'ckpt_path': None,\n",
    "    'step_two_model': 'models/reverse-step-wise-pivoting-de-nl-en-step-2.pt',\n",
    "})\n",
    "\n",
    "print('Experiment paramereters:')\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77661284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant directories.\n",
    "data_dir = os.path.join('./', 'data')\n",
    "tokenizers_dir = os.path.join('./', 'tokenizers')\n",
    "runs_dir = os.path.join('./', 'runs')\n",
    "\n",
    "# Experiment directories.\n",
    "run_dir = os.path.join(runs_dir, f'reverse-step-wise-pivoting-{hparams.src_lang}-{hparams.pvt_lang}-{hparams.tgt_lang}-{get_time_as_string()}')\n",
    "model_checkpoints_dir = os.path.join(run_dir, 'checkpoints')\n",
    "results_dir = os.path.join(run_dir, 'results')\n",
    "pre_training_eval_results_dir = os.path.join(run_dir, 'pre-training-eval-results')\n",
    "step_results_dir = os.path.join(run_dir, 'step-results')\n",
    "step_model_checkpoints_dir = os.path.join(run_dir, 'step_checkpoints')\n",
    "\n",
    "dirs = [data_dir, runs_dir, run_dir, model_checkpoints_dir, results_dir, pre_training_eval_results_dir, step_results_dir, step_model_checkpoints_dir]\n",
    "for dir in dirs:\n",
    "    create_dir(dir)\n",
    "\n",
    "print('Created directories.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Metrics.\n",
    "score_metric = load_metric('sacrebleu')\n",
    "\n",
    "print('Loaded metrics.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d5b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data.\n",
    "download_data(hparams.src_lang, hparams.tgt_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3318ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers.\n",
    "src_tokenizer = load_tokenizer(hparams.src_lang, hparams.tgt_lang)\n",
    "pvt_tokenizer = load_tokenizer(hparams.pvt_lang, hparams.tgt_lang)\n",
    "tgt_tokenizer = load_tokenizer(hparams.tgt_lang, hparams.src_lang)\n",
    "\n",
    "print('Loaded tokenizers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#\n",
    "#\n",
    "# !!!STEP 2!!! (step one is already done)\n",
    "# Freeze encoder and train src-pvt.\n",
    "#\n",
    "#\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "train_dataset, val_dataset, test_dataset = load_data(hparams.src_lang,\n",
    "                                                     hparams.pvt_lang,\n",
    "                                                     src_tokenizer,\n",
    "                                                     pvt_tokenizer,\n",
    "                                                     hparams.max_examples)\n",
    "\n",
    "print(f'Preprocessed data ({hparams.src_lang}-{hparams.pvt_lang})')\n",
    "print(f'\\tTraining data:   {len(train_dataset)}')\n",
    "print(f'\\tValidation data: {len(val_dataset)}')\n",
    "print(f'\\tTest data:       {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba18dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=hparams.batch_size, num_workers=hparams.num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=hparams.batch_size, num_workers=hparams.num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=hparams.batch_size, num_workers=hparams.num_workers)\n",
    "\n",
    "print('Created data loaders.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb021c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models.\n",
    "model = Transformer(src_tokenizer,\n",
    "                    pvt_tokenizer,\n",
    "                    score_metric=score_metric)\n",
    "\n",
    "pvt_tgt_model = Transformer(pvt_tokenizer,\n",
    "                            tgt_tokenizer,\n",
    "                            score_metric=score_metric)\n",
    "\n",
    "print('Created models.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192cc78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load model.\n",
    "pvt_tgt_model.load_state_dict(torch.load(hparams.pvt_tgt_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pre-trained decoder from step 1 and freeze its weights.\n",
    "model.decoder = pvt_tgt_model.decoder\n",
    "for param in model.decoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae1f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer.\n",
    "metric_logger = MetricLogger()\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "          dirpath=step_model_checkpoints_dir,\n",
    "          verbose=True,\n",
    "          save_last=True\n",
    "      )\n",
    "\n",
    "trainer = Trainer(deterministic=True,\n",
    "                  fast_dev_run=False,\n",
    "                  max_epochs=hparams.max_epochs,\n",
    "                  logger=metric_logger,\n",
    "                  log_every_n_steps=1,\n",
    "                  enable_checkpointing=True,\n",
    "                  default_root_dir=model_checkpoints_dir,\n",
    "                  callbacks=[checkpoint_callback],\n",
    "                  gpus=hparams.gpus if str(device) == 'cuda' else 0)\n",
    "\n",
    "print('Created trainer.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78dbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save untrained model.\n",
    "model_path = os.path.join(pre_training_eval_results_dir, 'model.pt')\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41535c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance.\n",
    "test_metrics = trainer.test(model, dataloaders=test_dataloader)\n",
    "print(test_metrics)\n",
    "metric_logger.manual_save(pre_training_eval_results_dir)\n",
    "metric_logger.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819125dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step 2.\n",
    "if hparams.step_two_model is None:\n",
    "    trainer.fit(model,\n",
    "            train_dataloaders=train_dataloader,\n",
    "            val_dataloaders=val_dataloader)\n",
    "else:\n",
    "    print('Loading step two model...')\n",
    "    model.load_state_dict(torch.load(hparams.step_two_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dcba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.\n",
    "step_model_path = os.path.join(run_dir, 'step-model.pt')\n",
    "torch.save(model.state_dict(), step_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing.\n",
    "test_metrics = trainer.test(model, dataloaders=test_dataloader)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b017d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss metrics.\n",
    "save_path = os.path.join(step_results_dir, 'loss.svg')\n",
    "plot_metric(metric_logger.metrics, 'loss', 'Loss', save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score metric.\n",
    "save_path = os.path.join(step_results_dir, 'score.svg')\n",
    "plot_metric(metric_logger.metrics, 'score', 'Score', save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e94cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save recorded metrics.\n",
    "metric_logger.manual_save(step_results_dir)\n",
    "metric_logger.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fdc2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#\n",
    "#\n",
    "# !!!FINE-TUNE STEP!!!\n",
    "# Fine tune on src-tgt\n",
    "#\n",
    "#\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dafab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "train_dataset, val_dataset, test_dataset = load_data(hparams.src_lang,\n",
    "                                                     hparams.tgt_lang,\n",
    "                                                     src_tokenizer,\n",
    "                                                     tgt_tokenizer,\n",
    "                                                     hparams.max_examples_fine_tune)\n",
    "\n",
    "print(f'Preprocessed data ({hparams.src_lang}-{hparams.tgt_lang})')\n",
    "print(f'\\tTraining data:   {len(train_dataset)}')\n",
    "print(f'\\tValidation data: {len(val_dataset)}')\n",
    "print(f'\\tTest data:       {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ac52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=hparams.batch_size, num_workers=hparams.num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=hparams.batch_size, num_workers=hparams.num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=hparams.batch_size, num_workers=hparams.num_workers)\n",
    "\n",
    "print('Created data loaders.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the encoder and fix the tokenizer and embedding.\n",
    "for param in model.decoder.parameters():\n",
    "    param.requires_grad = True\n",
    "model.tgt_tokenizer = tgt_tokenizer\n",
    "model.tgt_vocab_size = pvt_tgt_model.tgt_vocab_size\n",
    "model.tgt_embedding = pvt_tgt_model.tgt_embedding\n",
    "model.output_linear = pvt_tgt_model.output_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add aditional regularization to combat over-fitting on limited data.\n",
    "model.set_dropout_rate(0.3)\n",
    "model.weight_decay = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer.\n",
    "metric_logger = MetricLogger()\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "          dirpath=model_checkpoints_dir,\n",
    "          verbose=True,\n",
    "          save_last=True\n",
    "      )\n",
    "\n",
    "trainer = Trainer(deterministic=True,\n",
    "                  fast_dev_run=False,\n",
    "                  max_epochs=hparams.max_epochs,\n",
    "                  logger=metric_logger,\n",
    "                  log_every_n_steps=1,\n",
    "                  enable_checkpointing=True,\n",
    "                  default_root_dir=model_checkpoints_dir,\n",
    "                  callbacks=[checkpoint_callback],\n",
    "                  gpus=hparams.gpus if str(device) == 'cuda' else 0)\n",
    "\n",
    "print('Created trainer.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step 2.\n",
    "trainer.fit(model,\n",
    "            train_dataloaders=train_dataloader,\n",
    "            val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c022dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.\n",
    "model_path = os.path.join(run_dir, 'model.pt')\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faadbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing.\n",
    "test_metrics = trainer.test(model, dataloaders=test_dataloader)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c235a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss metrics.\n",
    "save_path = os.path.join(results_dir, 'loss.svg')\n",
    "plot_metric(metric_logger.metrics, 'loss', 'Loss', save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score metric.\n",
    "save_path = os.path.join(results_dir, 'score.svg')\n",
    "plot_metric(metric_logger.metrics, 'score', 'Score', save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hyper parameters.\n",
    "save_dict(run_dir, hparams, 'hparams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5e8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save recorded metrics.\n",
    "metric_logger.manual_save(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a80348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
