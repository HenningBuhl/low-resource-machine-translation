{"cells":[{"cell_type":"markdown","metadata":{"id":"KVsD8u--YG74"},"source":["# Evaluate Models on Benchmarks"],"id":"KVsD8u--YG74"},{"cell_type":"code","source":["# Static experiment settings.\n","experiment = 'benchmark'"],"metadata":{"id":"CKLFJadPMTSs"},"id":"CKLFJadPMTSs","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iXzLI0nkYG5u"},"source":["## Setup"],"id":"iXzLI0nkYG5u"},{"cell_type":"markdown","metadata":{"id":"GdK1e3SqYG1H"},"source":["### Environment"],"id":"GdK1e3SqYG1H"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e219183d"},"outputs":[],"source":["# If this is a notebook which is executed in colab [in_colab=True]:\n","#  1. Mount google drive and use the repository in there [mount_drive=True] (the repository must be in your google drive root folder).\n","#  2. Clone repository to remote machine [mount_drive=False].\n","in_colab = False\n","mount_drive = True\n","\n","try:\n","    # Check if running in colab.\n","    in_colab = 'google.colab' in str(get_ipython())\n","except:\n","    pass\n","\n","if in_colab:\n","    if mount_drive:\n","        # Mount google drive and navigate to it.\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","        %cd drive/MyDrive\n","    else:\n","        # Pull repository.\n","        !git clone https://github.com/HenningBuhl/low-resource-machine-translation\n","\n","    # Workaround for problem with undefined symbols (https://github.com/scverse/scvi-tools/issues/1464).\n","    !pip install --quiet scvi-colab\n","    from scvi_colab import install\n","    install()\n","\n","    # Navigate to the repository and install requirements.\n","    %cd low-resource-machine-translation\n","    !pip install -r requirements.txt\n","\n","    # Navigate to notebook location.\n","    %cd experiments"],"id":"e219183d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"aa380261"},"outputs":[],"source":["# Add src module directory to system path for subsecuent imports.\n","import sys\n","sys.path.insert(0, '../src')"],"id":"aa380261"},{"cell_type":"code","execution_count":null,"metadata":{"id":"843ca766"},"outputs":[],"source":["from util import is_notebook\n","\n","# Settings and module reloading (only in Jupyter Notebooks).\n","if is_notebook():\n","    # Module reloading.\n","    %load_ext autoreload\n","    %autoreload 2\n","\n","    # Plot settings.\n","    %matplotlib inline"],"id":"843ca766"},{"cell_type":"markdown","metadata":{"id":"t116PPgfTap1"},"source":["### Imports"],"id":"t116PPgfTap1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"L24eaww23CiU"},"outputs":[],"source":["# From packages.\n","import pytorch_lightning as pl\n","import argparse\n","from distutils.util import strtobool\n","\n","# From repository.\n","from arg_management import *\n","from benchmark import *\n","from constants import *\n","from data import *\n","from layers import *\n","from metric_logging import *\n","from plotting import *\n","from path_management import *\n","from tokenizer import *\n","from transformer import *\n","from util import *"],"id":"L24eaww23CiU"},{"cell_type":"markdown","metadata":{"id":"NDQW2rot7d0n"},"source":["### Arguments"],"id":"NDQW2rot7d0n"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6d5b0649"},"outputs":[],"source":["# Define arguments with argparse.\n","parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n","\n","# Experiment.\n","parser.add_argument('--inferece-methods', default='greedy', type=str, nargs=\"*\", choices=['greedy', 'beam-search', 'top-k', 'top-p'], help='The inference methods used.')\n","parser.add_argument('--beam-sizes', default=8, type=int, nargs=\"*\", help='The number of different beam sizes to be used.')\n","parser.add_argument('--top-ks', default=15, type=int, nargs=\"*\", help='The differnt top-Ks being used.')\n","parser.add_argument('--top-ps', default=0.7, type=int, nargs=\"*\", help='The differnt top-ps being used.')\n","\n","# Run.\n","arg_manager.add_run_args(parser)\n","# Metrics.\n","arg_manager.add_metrics_args(parser)\n","# Data.\n","arg_manager.add_data_args(parser)\n","# Tokenization.\n","arg_manager.add_tokenization_args(parser)\n","# Architecture.\n","arg_manager.add_architecture_args(parser)\n","# Optimizer.\n","arg_manager.add_optimizer_args(parser)\n","# Scheduler.\n","arg_manager.add_scheduler_args(parser)\n","# Training.\n","arg_manager.add_training_args(parser)\n","# Early Stopping + Model Checkpoint.\n","arg_manager.add_early_stopping_and_checkpoiting_args(parser)\n","\n","# Parse args.\n","if is_notebook():\n","    sys.argv = ['-f']  # Used to make argparse work in jupyter notebooks (all args must be optional).\n","    args, _ = parser.parse_known_args()  # -f can lead to unknown argument.\n","else:\n","    args = parser.parse_args()\n","\n","# Print args.\n","print('Arguments:')\n","print(args)"],"id":"6d5b0649"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VWCabcjvgiT"},"outputs":[],"source":["# Auto-infer args.\n","auto_infer_args(args, experiment)"],"id":"2VWCabcjvgiT"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5pEUXxmyTMmw"},"outputs":[],"source":["# Adjust arguments for test purposes.\n","if is_notebook() and True:  # Quickly turn on and off with 'and True/False'.\n","    #args.dev_run = True\n","    #args.fresh_run = True\n","\n","    args.max_epochs = 5\n","    args.batch_size = 1\n","    args.max_examples = 2\n","    args.num_val_examples = 1\n","    args.num_test_examples = 1\n","\n","    print('Adjusted args in notebook')"],"id":"5pEUXxmyTMmw"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jeeda_XMY5lZ"},"outputs":[],"source":["# Sanity check args.\n","sanity_check_args(args)"],"id":"Jeeda_XMY5lZ"},{"cell_type":"markdown","metadata":{"id":"OpSSObjs69g6"},"source":["### Seed"],"id":"OpSSObjs69g6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"oeC5Td1c69A7"},"outputs":[],"source":["# Set seed.\n","from pytorch_lightning import seed_everything\n","seed_everything(args.seed, workers=True)"],"id":"oeC5Td1c69A7"},{"cell_type":"markdown","metadata":{"id":"jcklieExTMm2"},"source":["### Paths"],"id":"jcklieExTMm2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKspH5la3Cid"},"outputs":[],"source":["# Create directories and create file names.\n","pm = ExperimentPathManager(experiment, experiment=experiment)\n","pm.init()"],"id":"jKspH5la3Cid"},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHCPZbsxJKFP"},"outputs":[],"source":["# Save arguments.\n","save_dict(pm.args_file, args.__dict__)"],"id":"WHCPZbsxJKFP"},{"cell_type":"code","source":[],"metadata":{"id":"XPMJf0RE3Dfu"},"id":"XPMJf0RE3Dfu","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9uBaXkm43DZl"},"id":"9uBaXkm43DZl","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"f4c0dVad3DXB"},"id":"f4c0dVad3DXB","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kaD_u3h83DUj"},"id":"kaD_u3h83DUj","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7VKRDxsh3DSC"},"id":"7VKRDxsh3DSC","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"844423ca","metadata":{"id":"844423ca"},"outputs":[],"source":["from util import is_notebook\n","\n","# Settings (only in Jupyter Notebooks).\n","if is_notebook():\n","    # Module reloading.\n","    %load_ext autoreload\n","    # aimport?\n","    %autoreload 2\n","    # Plot settings.\n","    %matplotlib inline"]},{"cell_type":"code","execution_count":null,"id":"060a4467","metadata":{"id":"060a4467"},"outputs":[],"source":["# Imports.\n","import pytorch_lightning as pl\n","from pytorch_lightning import Trainer\n","from torch.utils.data import DataLoader\n","from datasets import load_metric\n","\n","from constants import *\n","from util import *\n","from transformer import Transformer, cascaded_inference\n","from tokenizer import load_tokenizer\n","from data import download_data, load_data\n","from plotting import plot_metric\n","from metric_logging import MetricLogger\n","from benchmark import *"]},{"cell_type":"code","execution_count":null,"id":"fa8e7b47","metadata":{"id":"fa8e7b47"},"outputs":[],"source":["# Set seed.\n","from pytorch_lightning import seed_everything\n","seed_everything(0, workers=True)"]},{"cell_type":"code","execution_count":null,"id":"0283783c","metadata":{"id":"0283783c"},"outputs":[],"source":["# Experiment variables.\n","\n","# List of models to be evaluated.\n","model_configs = [\n","    # Baseline and cascaded.\n","    ModelConfig('single', ('de', 'en'), './models/baseline-de-en.pt'),\n","    ModelConfig('single', ('de', 'nl'), './models/baseline-de-nl.pt'),\n","    ModelConfig('single', ('nl', 'en'), './models/baseline-nl-en.pt'),\n","    ModelConfig('cascaded', ('de', 'nl', 'en'), ['./models/baseline-de-nl.pt', './models/baseline-nl-en.pt'], 'cascaded-de-nl-en'),\n","    # Baseline (limited de-en).\n","    ModelConfig('single', ('de', 'en'), './models/baseline-de-en-10000-examples.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/baseline-de-en-20000-examples.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/baseline-de-en-50000-examples.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/baseline-de-en-100000-examples.pt'),\n","    # Direct pivoting.\n","    ModelConfig('single', ('de', 'en'), './models/direct-pivoting-de-nl-en.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/direct-pivoting-de-nl-en-10000-examples.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/direct-pivoting-de-nl-en-20000-examples.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/direct-pivoting-de-nl-en-50000-examples.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/direct-pivoting-de-nl-en-100000-examples.pt'),\n","    # Step-wise pivoting.\n","    ModelConfig('single', ('de', 'en'), './models/step-wise-pivoting-de-nl-en.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/step-wise-pivoting-de-nl-en-10000-examples.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/step-wise-pivoting-de-nl-en-20000-examples.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/step-wise-pivoting-de-nl-en-50000-examples.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/step-wise-pivoting-de-nl-en-100000-examples.pt'),\n","    ModelConfig('single', ('nl', 'en'), './models/step-wise-pivoting-de-nl-en-step-2.pt'),\n","    # Reverse step-wise pivoting.\n","    ModelConfig('single', ('de', 'en'), './models/reverse-step-wise-pivoting-de-nl-en.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/reverse-step-wise-pivoting-de-nl-en-10000-examples.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/reverse-step-wise-pivoting-de-nl-en-20000-examples.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/reverse-step-wise-pivoting-de-nl-en-50000-examples.pt'),\n","    ModelConfig('single', ('de', 'en'), './models/reverse-step-wise-pivoting-de-nl-en-100000-examples.pt'),\n","    ModelConfig('single', ('de', 'nl'), './models/reverse-step-wise-pivoting-de-nl-en-step-2.pt'),\n","]\n","\n","# List of benchmarks to be applied to models.\n","benchmark_configs = [\n","    BenchmarkConfig('flores', flores_collate_fn, flores_pp_fn),\n","    BenchmarkConfig('tatoeba', tatoeba_collate_fn, tatoeba_pp_fn),\n","]"]},{"cell_type":"code","execution_count":null,"id":"428bec54","metadata":{"id":"428bec54"},"outputs":[],"source":["# Constant directories.\n","data_dir = os.path.join('./', 'data')\n","tokenizers_dir = os.path.join('./', 'tokenizers')\n","runs_dir = os.path.join('./', 'runs')\n","\n","# Experiment directories.\n","run_dir = os.path.join(runs_dir, f'benchmark-{get_time_as_string()}')\n","\n","dirs = [data_dir, tokenizers_dir, runs_dir, run_dir]\n","for dir in dirs:\n","    create_dir(dir)\n","\n","print('Created directories.')"]},{"cell_type":"code","execution_count":null,"id":"d4338e51","metadata":{"scrolled":true,"id":"d4338e51"},"outputs":[],"source":["# Load Metrics.\n","score_metric = load_metric('sacrebleu')\n","\n","print('Loaded metrics.')"]},{"cell_type":"code","execution_count":null,"id":"d2e00bc4","metadata":{"id":"d2e00bc4"},"outputs":[],"source":["# Experiment paramereters.\n","hparams = dotdict({\n","    'beam_size': 8,\n","    'top_k': 15,\n","    'top_p': 0.6,\n","    'num_workers': 4,\n","})\n","\n","print('Experiment paramereters:')\n","print(hparams)"]},{"cell_type":"code","execution_count":null,"id":"6d016f19","metadata":{"id":"6d016f19"},"outputs":[],"source":["# Perform benchmark...\n","for bc in benchmark_configs:\n","    print(f'Performing \"{bc.name}\" benchmark on {len(model_configs)} models.')\n","    \n","    # Create directories.\n","    benchmark_dir = os.path.join(run_dir, bc.name)\n","    dirs = [benchmark_dir]\n","    for dir in dirs:\n","        create_dir(dir)\n","    \n","    # Download and unpack data.\n","    bc.collate_fn(data_dir)\n","    \n","    # ... on every model.\n","    for mc in model_configs:\n","        print(f'\\tBenchmarking {mc.name} ({mc.langs}) on {bc.name}.')\n","        \n","        # Create directories.\n","        results_dir = os.path.join(benchmark_dir, mc.name)\n","        dirs = [results_dir]\n","        for dir in dirs:\n","            create_dir(dir)\n","        \n","        method_kwargs = {\n","            'greedy': {},\n","            'beam': {'beam_size': hparams.beam_size},\n","            'top_k': {'top_k': hparams.top_k},\n","            'top_p': {'top_p': hparams.top_p},\n","        }\n","        test_results = {k: 0 for k in method_kwargs.keys()}\n","        for method, kwargs in method_kwargs.items():\n","            print(f'\\t\\tBenchmarking {mc.name} ({mc.langs}) on {bc.name} with inference method {method}.')\n","\n","            # Perform evaluation base on mc.type.\n","            if mc.type == 'single':\n","                src_lang, tgt_lang = mc.langs\n","\n","                # Load tokenizers.\n","                src_tokenizer = load_tokenizer(src_lang, tgt_lang)\n","                tgt_tokenizer = load_tokenizer(tgt_lang, src_lang)\n","                print('\\t\\tLoaded tokenizers.')\n","\n","                # Load data.\n","                dataset = bc.pp_fn(data_dir, src_lang, tgt_lang, src_tokenizer, tgt_tokenizer)\n","                print('\\t\\tLoaded data.')\n","\n","                # Create dataloader.\n","                test_dataloader = DataLoader(dataset, batch_size=1, num_workers=hparams.num_workers)\n","                print('\\t\\tCreated data loader.')\n","\n","                # Create model.\n","                model = Transformer(src_tokenizer,\n","                        tgt_tokenizer,\n","                        score_metric=score_metric)\n","                print('\\t\\tCreated model.')\n","\n","                # Load model.\n","                model.load_state_dict(torch.load(mc.paths))\n","                model.to(device)\n","                print('\\t\\tLoaded model.')\n","\n","                # Testing.\n","                for batch_idx, batch in enumerate(test_dataloader):\n","                    src_input, tgt_input, tgt_output = batch \n","                    \n","                    # Convert preprocessed input back to text.\n","                    src_text = src_tokenizer.Decode(src_input.tolist())[0]\n","                    label_text = tgt_tokenizer.Decode(tgt_input.tolist())[0]\n","\n","                    # Pass through model.\n","                    tgt_text = model.translate(src_text, method='sampling' if 'top' in method else method, kwargs=kwargs)\n","\n","                    # Calculate metrics.\n","                    score = score_metric.compute(predictions=[tgt_text], references=[[label_text]])['score']\n","                    \n","                    # Accumulate metrics.\n","                    test_results[method] += score\n","                    print(f'\\t\\t{score}')\n","                test_results[method] = [test_results[method] / len(dataset)]\n","\n","            elif mc.type == 'cascaded':\n","                src_lang, pvt_lang, tgt_lang = mc.langs\n","\n","                # Load tokenizers.\n","                src_tokenizer = load_tokenizer(src_lang, tgt_lang)\n","                pvt_tokenizer = load_tokenizer(pvt_lang, tgt_lang)\n","                tgt_tokenizer = load_tokenizer(tgt_lang, src_lang)\n","                print('\\t\\tLoaded tokenizers.')\n","\n","                # Load data.\n","                dataset = bc.pp_fn(data_dir, src_lang, tgt_lang, src_tokenizer, tgt_tokenizer)\n","                print('\\t\\tLoaded data.')\n","\n","                # Create dataloader.\n","                test_dataloader = DataLoader(dataset, batch_size=1, num_workers=hparams.num_workers)\n","                print('\\t\\tCreated data loader.')\n","\n","                # Create model.\n","                src_pvt_model = Transformer(src_tokenizer,\n","                                            pvt_tokenizer,\n","                                            score_metric=score_metric)\n","                pvt_tgt_model = Transformer(pvt_tokenizer,\n","                                            tgt_tokenizer,\n","                                            score_metric=score_metric)\n","                print('\\t\\tCreated models.')\n","\n","                # Load model.\n","                src_pvt_model.load_state_dict(torch.load(mc.paths[0]))\n","                pvt_tgt_model.load_state_dict(torch.load(mc.paths[1]))\n","                src_pvt_model.to(device)\n","                pvt_tgt_model.to(device)\n","                print('\\t\\tLoaded models.')\n","\n","                # Testing.\n","                for batch_idx, batch in enumerate(test_dataloader):\n","                    # Cascaded inference.\n","                    score, src_text, pvt_text, tgt_text, label_text = cascaded_inference(batch,\n","                                                                                         src_tokenizer, tgt_tokenizer,\n","                                                                                         src_pvt_model, pvt_tgt_model,\n","                                                                                         score_metric,\n","                                                                                         method='sampling' if 'top' in method else method,\n","                                                                                         kwargs=kwargs)\n","                    # Accumulate metrics.\n","                    test_results[method] += score\n","                    print(f'\\t\\t{score}')\n","                test_results[method] = [test_results[method] / len(dataset)]\n","\n","            else:\n","                raise ValueError(f'Unknown model_config.type: {mc.type}')\n","\n","        # Save recorded metrics.\n","        save_dict(results_dir, test_results, 'metrics')"]},{"cell_type":"code","execution_count":null,"id":"594195d5","metadata":{"id":"594195d5"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"e16c6ae93e97f8b72f7c60789e33aa05f059bdee2efc0781f4556fc1fbea3a2d"}},"colab":{"private_outputs":true,"provenance":[],"toc_visible":true,"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}