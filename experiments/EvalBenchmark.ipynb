{"cells":[{"cell_type":"markdown","id":"KVsD8u--YG74","metadata":{"id":"KVsD8u--YG74"},"source":["# Evaluate Models on Benchmarks"]},{"cell_type":"code","execution_count":null,"id":"CKLFJadPMTSs","metadata":{"id":"CKLFJadPMTSs"},"outputs":[],"source":["# Static experiment settings.\n","experiment = 'benchmark'"]},{"cell_type":"markdown","id":"iXzLI0nkYG5u","metadata":{"id":"iXzLI0nkYG5u"},"source":["## Setup"]},{"cell_type":"markdown","id":"GdK1e3SqYG1H","metadata":{"id":"GdK1e3SqYG1H"},"source":["### Environment"]},{"cell_type":"code","execution_count":null,"id":"e219183d","metadata":{"id":"e219183d"},"outputs":[],"source":["# If this is a notebook which is executed in colab [in_colab=True]:\n","#  1. Mount google drive and use the repository in there [mount_drive=True] (the repository must be in your google drive root folder).\n","#  2. Clone repository to remote machine [mount_drive=False].\n","in_colab = False\n","mount_drive = True\n","\n","try:\n","    # Check if running in colab.\n","    in_colab = 'google.colab' in str(get_ipython())\n","except:\n","    pass\n","\n","if in_colab:\n","    if mount_drive:\n","        # Mount google drive and navigate to it.\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","        %cd drive/MyDrive\n","    else:\n","        # Pull repository.\n","        !git clone https://github.com/HenningBuhl/low-resource-machine-translation\n","\n","    # Workaround for problem with undefined symbols (https://github.com/scverse/scvi-tools/issues/1464).\n","    !pip install --quiet scvi-colab\n","    from scvi_colab import install\n","    install()\n","\n","    # Navigate to the repository and install requirements.\n","    %cd low-resource-machine-translation\n","    !pip install -r requirements.txt\n","\n","    # Navigate to notebook location.\n","    %cd experiments"]},{"cell_type":"code","execution_count":null,"id":"aa380261","metadata":{"id":"aa380261"},"outputs":[],"source":["# Add src module directory to system path for subsecuent imports.\n","import sys\n","sys.path.insert(0, '../src')"]},{"cell_type":"code","execution_count":null,"id":"843ca766","metadata":{"id":"843ca766"},"outputs":[],"source":["from util import is_notebook\n","\n","# Settings and module reloading (only in Jupyter Notebooks).\n","if is_notebook():\n","    # Module reloading.\n","    %load_ext autoreload\n","    %autoreload 2\n","\n","    # Plot settings.\n","    %matplotlib inline"]},{"cell_type":"markdown","id":"t116PPgfTap1","metadata":{"id":"t116PPgfTap1"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"id":"L24eaww23CiU","metadata":{"id":"L24eaww23CiU"},"outputs":[],"source":["# From packages.\n","import pytorch_lightning as pl\n","import argparse\n","from distutils.util import strtobool\n","\n","# From repository.\n","from arg_management import *\n","from constants import *\n","from data import *\n","from layers import *\n","from metric_logging import *\n","from plotting import *\n","from path_management import *\n","from tokenizer import *\n","from transformer import *\n","from util import *"]},{"cell_type":"markdown","id":"NDQW2rot7d0n","metadata":{"id":"NDQW2rot7d0n"},"source":["### Arguments"]},{"cell_type":"code","execution_count":null,"id":"6d5b0649","metadata":{"id":"6d5b0649"},"outputs":[],"source":["# Define arguments with argparse.\n","parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n","\n","# Experiment.\n","parser.add_argument('--seed', default=0, type=int, help='The random seed of the program.')\n","parser.add_argument('--inferece-methods', default='greedy', type=str, nargs=\"*\", choices=['greedy', 'beam-search', 'top-k', 'top-p'], help='The inference methods used.')\n","parser.add_argument('--beam-size', default=8, type=int, nargs=\"*\", help='The number of different beam sizes to be used.')\n","parser.add_argument('--top-k', default=15, type=int, nargs=\"*\", help='The differnt top-Ks being used.')\n","parser.add_argument('--top-p', default=0.7, type=int, nargs=\"*\", help='The differnt top-ps being used.')\n","\n","# Metrics.\n","arg_manager.add_metrics_args(parser)\n","\n","# Parse args.\n","if is_notebook():\n","    sys.argv = ['-f']  # Used to make argparse work in jupyter notebooks (all args must be optional).\n","    args, _ = parser.parse_known_args()  # -f can lead to unknown argument.\n","else:\n","    args = parser.parse_args()\n","\n","# Print args.\n","print('Arguments:')\n","print(args)"]},{"cell_type":"code","execution_count":null,"id":"2VWCabcjvgiT","metadata":{"id":"2VWCabcjvgiT"},"outputs":[],"source":["# Auto-infer args.\n","auto_infer_args(args, experiment)"]},{"cell_type":"code","execution_count":null,"id":"5pEUXxmyTMmw","metadata":{"id":"5pEUXxmyTMmw"},"outputs":[],"source":["# Adjust arguments for test purposes.\n","if is_notebook() and True:  # Quickly turn on and off with 'and True/False'.\n","    #args.dev_run = True\n","    #args.fresh_run = True\n","\n","    print('Adjusted args in notebook')"]},{"cell_type":"code","execution_count":null,"id":"Jeeda_XMY5lZ","metadata":{"id":"Jeeda_XMY5lZ"},"outputs":[],"source":["# Sanity check args.\n","sanity_check_args(args)"]},{"cell_type":"markdown","id":"OpSSObjs69g6","metadata":{"id":"OpSSObjs69g6"},"source":["### Seed"]},{"cell_type":"code","execution_count":null,"id":"oeC5Td1c69A7","metadata":{"id":"oeC5Td1c69A7"},"outputs":[],"source":["# Set seed.\n","from pytorch_lightning import seed_everything\n","seed_everything(args.seed, workers=True)"]},{"cell_type":"markdown","id":"jcklieExTMm2","metadata":{"id":"jcklieExTMm2"},"source":["### Paths"]},{"cell_type":"code","execution_count":null,"id":"Y-QXsIWgUeJp","metadata":{"id":"Y-QXsIWgUeJp"},"outputs":[],"source":["# Read and create directories and files.\n","bm = BenchmarkManager()\n","bm.init()"]},{"cell_type":"code","execution_count":null,"id":"WHCPZbsxJKFP","metadata":{"id":"WHCPZbsxJKFP"},"outputs":[],"source":["# Save arguments.\n","save_dict(bm.args_file, args.__dict__)"]},{"cell_type":"markdown","id":"eWi44WbRWUat","metadata":{"id":"eWi44WbRWUat"},"source":["## Benchmark"]},{"cell_type":"code","source":["# Create run dir.\n","run_dir = os.path.join(CONST_RUNS_DIR, f'benchmark-{get_time_as_string()}')\n","create_dir(run_dir)\n","\n","# Which metrics to record.\n","track_metrics = []  # TODO from args."],"metadata":{"id":"lbpQ4reyRrto"},"id":"lbpQ4reyRrto","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"XPMJf0RE3Dfu","metadata":{"id":"XPMJf0RE3Dfu"},"outputs":[],"source":["# Iterate over benchmarks.\n","for benchmark_name in get_dirs(CONST_BENCHMARKS_DIR):\n","    print(f'Benchmark: {benchmark_name}')\n","\n","    # Create benchmark result dir.\n","    benchmark_dir = os.path.join(run_dir, benchmark_name)\n","    create_dir(benchmark_dir)\n","\n","    # Load benchmark data.\n","    # TODO (with BenchmarkDataPreProcessor [automatically detect /de,/en or /de-en])\n","\n","    # Iterate over models.\n","    for model_name in get_dirs(CONST_MODELS_DIR):\n","        print(f'Model: {model_name}')\n","\n","        # Create model model dir.\n","        model_dir = os.path.join(run_dir, model_name)\n","        create_dir(model_dir)\n","\n","        # Load model args.\n","        args = load_dict(os.path.join(CONST_MODELS_DIR, model_name, 'args.json'))\n","\n","        # Load tokenizers and model(s).\n","        experiment_type = args['experiment']\n","        if experiment_type == 'cascaded':\n","            # Load tokenizers.\n","            # TODO\n","\n","            # Load model(s).\n","            # TODO\n","\n","            # Create function that translates input text and returns it (experiment_type agnostic for further code below).\n","            # TODO\n","            pass\n","        else:\n","            pass\n","\n","        # Iterate over inference methods.\n","        for method_name in args.inference_methods:\n","            print(f'Method: {method_name}')\n","\n","            # Iterate over inference method params.\n","            values = ... args... # None for greedy?\n","            for value in values:\n","                #print(f'Value: {value}')\n","                ...\n","\n","                # Perform inference.\n","                # TODO\n","\n","                # Calculate metrics.\n","                metrics = {}\n","                # TODO\n","\n","                # Save metrics to separate file each.\n","                # TODO\n","                # for metric in track_metrics\n","                #     TODO calculate...\n","                #     os.path.join(model_dir, method_name, f'{'' if value is None else (value+\"/\")}{metric}.json')\n","                #     value = metrics[metric]"]},{"cell_type":"code","execution_count":null,"id":"9uBaXkm43DZl","metadata":{"id":"9uBaXkm43DZl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f4c0dVad3DXB","metadata":{"id":"f4c0dVad3DXB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"kaD_u3h83DUj","metadata":{"id":"kaD_u3h83DUj"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"7VKRDxsh3DSC","metadata":{"id":"7VKRDxsh3DSC"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"6d016f19","metadata":{"id":"6d016f19"},"outputs":[],"source":["# Perform benchmark...\n","for bc in benchmark_configs:\n","    print(f'Performing \"{bc.name}\" benchmark on {len(model_configs)} models.')\n","    \n","    # Create directories.\n","    benchmark_dir = os.path.join(run_dir, bc.name)\n","    dirs = [benchmark_dir]\n","    for dir in dirs:\n","        create_dir(dir)\n","    \n","    # Download and unpack data.\n","    bc.collate_fn(data_dir)\n","    \n","    # ... on every model.\n","    for mc in model_configs:\n","        print(f'\\tBenchmarking {mc.name} ({mc.langs}) on {bc.name}.')\n","        \n","        # Create directories.\n","        results_dir = os.path.join(benchmark_dir, mc.name)\n","        dirs = [results_dir]\n","        for dir in dirs:\n","            create_dir(dir)\n","        \n","        method_kwargs = {\n","            'greedy': {},\n","            'beam': {'beam_size': hparams.beam_size},\n","            'top_k': {'top_k': hparams.top_k},\n","            'top_p': {'top_p': hparams.top_p},\n","        }\n","        test_results = {k: 0 for k in method_kwargs.keys()}\n","        for method, kwargs in method_kwargs.items():\n","            print(f'\\t\\tBenchmarking {mc.name} ({mc.langs}) on {bc.name} with inference method {method}.')\n","\n","            # Perform evaluation base on mc.type.\n","            if mc.type == 'single':\n","                src_lang, tgt_lang = mc.langs\n","\n","                # Load tokenizers.\n","                src_tokenizer = load_tokenizer(src_lang, tgt_lang)\n","                tgt_tokenizer = load_tokenizer(tgt_lang, src_lang)\n","                print('\\t\\tLoaded tokenizers.')\n","\n","                # Load data.\n","                dataset = bc.pp_fn(data_dir, src_lang, tgt_lang, src_tokenizer, tgt_tokenizer)\n","                print('\\t\\tLoaded data.')\n","\n","                # Create dataloader.\n","                test_dataloader = DataLoader(dataset, batch_size=1, num_workers=hparams.num_workers)\n","                print('\\t\\tCreated data loader.')\n","\n","                # Create model.\n","                model = Transformer(src_tokenizer,\n","                        tgt_tokenizer,\n","                        score_metric=score_metric)\n","                print('\\t\\tCreated model.')\n","\n","                # Load model.\n","                model.load_state_dict(torch.load(mc.paths))\n","                model.to(device)\n","                print('\\t\\tLoaded model.')\n","\n","                # Testing.\n","                for batch_idx, batch in enumerate(test_dataloader):\n","                    src_input, tgt_input, tgt_output = batch \n","                    \n","                    # Convert preprocessed input back to text.\n","                    src_text = src_tokenizer.Decode(src_input.tolist())[0]\n","                    label_text = tgt_tokenizer.Decode(tgt_input.tolist())[0]\n","\n","                    # Pass through model.\n","                    tgt_text = model.translate(src_text, method='sampling' if 'top' in method else method, kwargs=kwargs)\n","\n","                    # Calculate metrics.\n","                    score = score_metric.compute(predictions=[tgt_text], references=[[label_text]])['score']\n","                    \n","                    # Accumulate metrics.\n","                    test_results[method] += score\n","                    print(f'\\t\\t{score}')\n","                test_results[method] = [test_results[method] / len(dataset)]\n","\n","            elif mc.type == 'cascaded':\n","                src_lang, pvt_lang, tgt_lang = mc.langs\n","\n","                # Load tokenizers.\n","                src_tokenizer = load_tokenizer(src_lang, tgt_lang)\n","                pvt_tokenizer = load_tokenizer(pvt_lang, tgt_lang)\n","                tgt_tokenizer = load_tokenizer(tgt_lang, src_lang)\n","                print('\\t\\tLoaded tokenizers.')\n","\n","                # Load data.\n","                dataset = bc.pp_fn(data_dir, src_lang, tgt_lang, src_tokenizer, tgt_tokenizer)\n","                print('\\t\\tLoaded data.')\n","\n","                # Create dataloader.\n","                test_dataloader = DataLoader(dataset, batch_size=1, num_workers=hparams.num_workers)\n","                print('\\t\\tCreated data loader.')\n","\n","                # Create model.\n","                src_pvt_model = Transformer(src_tokenizer,\n","                                            pvt_tokenizer,\n","                                            score_metric=score_metric)\n","                pvt_tgt_model = Transformer(pvt_tokenizer,\n","                                            tgt_tokenizer,\n","                                            score_metric=score_metric)\n","                print('\\t\\tCreated models.')\n","\n","                # Load model.\n","                src_pvt_model.load_state_dict(torch.load(mc.paths[0]))\n","                pvt_tgt_model.load_state_dict(torch.load(mc.paths[1]))\n","                src_pvt_model.to(device)\n","                pvt_tgt_model.to(device)\n","                print('\\t\\tLoaded models.')\n","\n","                # Testing.\n","                for batch_idx, batch in enumerate(test_dataloader):\n","                    # Cascaded inference.\n","                    score, src_text, pvt_text, tgt_text, label_text = cascaded_inference(batch,\n","                                                                                         src_tokenizer, tgt_tokenizer,\n","                                                                                         src_pvt_model, pvt_tgt_model,\n","                                                                                         score_metric,\n","                                                                                         method='sampling' if 'top' in method else method,\n","                                                                                         kwargs=kwargs)\n","                    # Accumulate metrics.\n","                    test_results[method] += score\n","                    print(f'\\t\\t{score}')\n","                test_results[method] = [test_results[method] / len(dataset)]\n","\n","            else:\n","                raise ValueError(f'Unknown model_config.type: {mc.type}')\n","\n","        # Save recorded metrics.\n","        save_dict(results_dir, test_results, 'metrics')"]},{"cell_type":"code","execution_count":null,"id":"594195d5","metadata":{"id":"594195d5"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"private_outputs":true,"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"e16c6ae93e97f8b72f7c60789e33aa05f059bdee2efc0781f4556fc1fbea3a2d"}}},"nbformat":4,"nbformat_minor":5}